# Datasets and Evaluation Metrics

The provided fine tuning script allows to select between three datasets by passing pass the `dataset` arg to the `llama_finetuning.py` script. The current options are `grammar_dataset`, `alpaca_dataset`, and `cnn_dailymail_dataset`.

* [grammar_dataset](https://huggingface.co/datasets/jfleg) contains 150K pairs of english sentences and possible corrections.
* [alpaca_dataset](https://github.com/tatsu-lab/stanford_alpaca) provides 52K instruction-response pairs as generated by `text-davinci-003`.
* [`cnn_dailymail_dataset`](https://huggingface.co/datasets/cnn_dailymail) provides over 300k news article together with their summarizaton.

## Application 
In the following we list other datasets and their main use cases that can be used for fine tuning.

### Q&A these can be used for evaluation as well
- [MMLU](https://huggingface.co/datasets/lukaemon/mmlu/viewer/astronomy/validation)

- [BoolQ](https://huggingface.co/datasets/boolq)
- [NarrativeQA](https://huggingface.co/datasets/narrativeqa)
- [NaturalQuestions](https://huggingface.co/datasets/natural_questions) (closed-book)
- [NaturalQuestions](https://huggingface.co/datasets/openbookqa) (open-book)
- QuAC
- HellaSwag
- OpenbookQA
- TruthfulQA ( can be helpful for fact checking/ misinformation of the model)


### Summarization
- [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)
- [XSUM](https://huggingface.co/datasets/xsum)


### instruction finetuning 
- [Alpaca](https://huggingface.co/datasets/yahma/alpaca-cleaned)	52k	instruction tuning
- [Dolly](https://github.com/databrickslabs/dolly/tree/master/data) 15k	15k	instruction tuning


### simple text generation for quick tests
[English](https://huggingface.co/datasets/Abirate/english_quotes) quotes	2508	Multi-label text classification, text generation


### Reasoning used mostly for evaluation of LLMs
- Synthetic reasoning (abstract symbols)
- Synthetic reasoning (natural language)
- bAbI
- Dyck
- GSM8K
- MATH
- MATH (chain-of-thoughts)
- APPS (Code)
- HumanEval (Code)
- LSAT
- LegalSupport
- Data imputation
- Entity matching

### Toxity evlauation 
- [Real_toxic_prompts](https://huggingface.co/datasets/allenai/real-toxicity-prompts)

### Bias evlauation 
- [Crows_pair] (https://huggingface.co/datasets/crows_pairs) gender bias
- [WinoGender] gender bias

### Useful Links
More info on evalation dataset can be found in [HELM](https://crfm.stanford.edu/helm/latest/)