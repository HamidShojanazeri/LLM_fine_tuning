# Datasets and Evaluation Metrics

The provided fine tuning script allows to select between three datasets by passing pass the `dataset` arg to the `llama_finetuning.py` script. The current options are `grammar_dataset`, `alpaca_dataset`, and `cnn_dailymail_dataset`.

* [grammar_dataset](https://huggingface.co/datasets/jfleg) contains 150K pairs of english sentences and possible corrections.
* [alpaca_dataset](https://github.com/tatsu-lab/stanford_alpaca) provides 52K instruction-response pairs as generated by `text-davinci-003`.
* [`cnn_dailymail_dataset`](https://huggingface.co/datasets/cnn_dailymail) provides over 300k news article together with their summarizaton.

## Adding custom datasets

The list of available datasets can be easily extended with custom datasets by following this instruction.

Each dataset has a corresponding configuration (dataclass) in [configs/dataset.py](../configs/dataset.py) which contains the dataset name, training/validation split names, as well as optional parameters like datafiles etc.

Additionally, there is a preprocessing function for each dataset in the [ft_datasets](../ft_datasets) folder.
The returned data of the dataset needs to be consumable by the forward method of the fine-tuned model by calling ```model(**data)```.
For CausalLM models this usually means that the data needs to be in the form of a dictionary with "input_ids", "attention_mask" and "labels" fields.

To add a custom dataset the following steps need to be performed.

1. Create a dataset configuration after the schema described above. Examples can be found in [configs/dataset.py](../configs/dataset.py).
2. Create a preprocessing routine which loads the data and returns a PyTorch style dataset. The signature for the preprocessing function needs to be (dataset_config, tokenizer, split_name) where split_name will be the string for train/validation split as defined in the dataclass.
3. Register the dataset name and preprocessing function by inserting it as key and value into the DATASET_PREPROC dictionary in [utils/dataset_utils.py](../utils/dataset_utils.py)
4. Set dataset field in training config to dataset name or use --dataset option of the llama_finetuning.py training script.

## Application 
In the following we list other datasets and their main use cases that can be used for fine tuning.

### Q&A these can be used for evaluation as well
- [MMLU](https://huggingface.co/datasets/lukaemon/mmlu/viewer/astronomy/validation)

- [BoolQ](https://huggingface.co/datasets/boolq)
- [NarrativeQA](https://huggingface.co/datasets/narrativeqa)
- [NaturalQuestions](https://huggingface.co/datasets/natural_questions) (closed-book)
- [NaturalQuestions](https://huggingface.co/datasets/openbookqa) (open-book)
- QuAC
- HellaSwag
- OpenbookQA
- TruthfulQA ( can be helpful for fact checking/ misinformation of the model)


### Summarization
- [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)
- [XSUM](https://huggingface.co/datasets/xsum)


### instruction finetuning 
- [Alpaca](https://huggingface.co/datasets/yahma/alpaca-cleaned)	52k	instruction tuning
- [Dolly](https://github.com/databrickslabs/dolly/tree/master/data) 15k	15k	instruction tuning


### simple text generation for quick tests
[English](https://huggingface.co/datasets/Abirate/english_quotes) quotes	2508	Multi-label text classification, text generation


### Reasoning used mostly for evaluation of LLMs
- Synthetic reasoning (abstract symbols)
- Synthetic reasoning (natural language)
- bAbI
- Dyck
- GSM8K
- MATH
- MATH (chain-of-thoughts)
- APPS (Code)
- HumanEval (Code)
- LSAT
- LegalSupport
- Data imputation
- Entity matching

### Toxity evlauation 
- [Real_toxic_prompts](https://huggingface.co/datasets/allenai/real-toxicity-prompts)

### Bias evlauation 
- [Crows_pair] (https://huggingface.co/datasets/crows_pairs) gender bias
- [WinoGender] gender bias

### Useful Links
More info on evalation dataset can be found in [HELM](https://crfm.stanford.edu/helm/latest/)